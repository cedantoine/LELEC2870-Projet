{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import zscore\n",
    "\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    use_seaborn = True\n",
    "    sns.set()\n",
    "except:\n",
    "    use_seaborn = False\n",
    "\n",
    "showfig = False\n",
    "delOutliers = False\n",
    "downSizing  = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data \n",
    "X1 = pd.read_csv(\"X1.csv\")\n",
    "Y1 = pd.read_csv(\"Y1.csv\",header=None,names =['shares'])\n",
    "\n",
    "if showfig:\n",
    "    fig = plt.figure(figsize=(6.4*2,4.8*6))\n",
    "    gs  = fig.add_gridspec(nrows=12, ncols=5)\n",
    "    for (i,header) in enumerate(X1.columns):\n",
    "        ax = fig.add_subplot(gs[int(i/5),i%5])\n",
    "        ax.scatter(X1[header],Y1.values, s=5)\n",
    "        ax.set_xlabel(header)\n",
    "    fig.tight_layout()\n",
    "\n",
    "X1_val = X1.values\n",
    "Y1_val = Y1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if delOutliers:\n",
    "#Removing outliers\n",
    "    z_scores = zscore(X1_val)\n",
    "    abs_z_scores = np.abs(z_scores)\n",
    "    X1_filtered_ind = (abs_z_scores < 4).all(axis=1)\n",
    "    X1_val = X1_val[X1_filtered_ind]\n",
    "    Y1_val = Y1_val[X1_filtered_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if downSizing:\n",
    "    # corr = np.corrcoef(X1_train)\n",
    "    # corr = np.triu(corr) - np.eye(corr.shape[0]) #np.diag(np.diag(corr))\n",
    "    # delt = np.where(corr>1-1e-7)\n",
    "\n",
    "    # keep = np.where([idx not in delt[0] for idx in range(corr.shape[0])])[0]\n",
    "    # print(X1_val.shape[0], len(keep))\n",
    "\n",
    "    # X1_train,Y1_train = X1_train[keep,:],Y1_train[keep,:]\n",
    "    \n",
    "    corr = np.corrcoef(X1_val.transpose())\n",
    "    corr = np.triu(corr) - np.eye(corr.shape[0]) #np.diag(np.diag(corr))\n",
    "    delt = np.where(corr>1-1e-1)\n",
    "\n",
    "    keep = np.where([idx not in delt[0] for idx in range(corr.shape[0])])[0]\n",
    "    print(X1_val.shape[1], len(keep))\n",
    "\n",
    "    X1_val = X1_val[:,keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score computation\n",
    "def scoref1(ytrue, ypred, th):\n",
    "    return sklearn.metrics.f1_score(ytrue>th, ypred>th)\n",
    "\n",
    "def scoreregression(ytrue, ypred):\n",
    "    scores = [\n",
    "        scoref1(ytrue, ypred, th=th) for th in [ 500, 1400, 5000, 10000]\n",
    "    ]\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selections\n",
    "def correlation_selection(X,Y,nb):\n",
    "    data = np.concatenate((np.transpose(X),np.transpose(Y)))\n",
    "    corr = np.corrcoef(data)[:,-1]\n",
    "    corr_bis=corr[:-1]\n",
    "    idxs = np.argpartition(corr_bis, -nb)[-nb:]\n",
    "    X_corr = X[:,idxs]\n",
    "    return X_corr\n",
    "\n",
    "def PCA_selection(X,nb):\n",
    "    # To do 1: initiate StandardScaler class\n",
    "    scaler = StandardScaler(copy=True,with_mean=True,with_std=True)\n",
    "    # Initialize PCA\n",
    "    pca = PCA(n_components=nb)\n",
    "    X_pca = pca.fit_transform(scaler.fit_transform(X))\n",
    "    \n",
    "    return X_pca\n",
    "\n",
    "def mutual_info_selection(X,Y,nb):\n",
    "    mutual_information = mutual_info_regression(X,Y)\n",
    "    idxs = np.argpartition(mutual_information,-nb)[-nb:]\n",
    "    X_mutual_info = X[:,idxs[:]]\n",
    "\n",
    "    return X_mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_correlation_10=correlation_selection(X1_val,Y1_val,10)\n",
    "X_correlation_12=correlation_selection(X1_val,Y1_val,12)\n",
    "X_correlation_15=correlation_selection(X1_val,Y1_val,15)\n",
    "X_correlation_17=correlation_selection(X1_val,Y1_val,17)\n",
    "X_correlation_20=correlation_selection(X1_val,Y1_val,20)\n",
    "\n",
    "X_correlation=[X_correlation_10,X_correlation_12,X_correlation_15,X_correlation_17,X_correlation_20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_10=PCA_selection(X1_val,10)\n",
    "X_pca_12=PCA_selection(X1_val,12)\n",
    "X_pca_15=PCA_selection(X1_val,15)\n",
    "X_pca_17=PCA_selection(X1_val,17)\n",
    "X_pca_20=PCA_selection(X1_val,20)\n",
    "\n",
    "X_pca=[X_pca_10,X_pca_12,X_pca_15,X_pca_17,X_pca_20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_mutual_info_10=mutual_info_selection(X1_val,Y1_val,10)\n",
    "X_mutual_info_12=mutual_info_selection(X1_val,Y1_val,12)\n",
    "X_mutual_info_15=mutual_info_selection(X1_val,Y1_val,15)\n",
    "X_mutual_info_17=mutual_info_selection(X1_val,Y1_val,17)\n",
    "X_mutual_info_20=mutual_info_selection(X1_val,Y1_val,20)\n",
    "\n",
    "X_mutual_info=[X_mutual_info_10,X_mutual_info_12,X_mutual_info_15,X_mutual_info_17,X_mutual_info_20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X,Y,f_selection):\n",
    "    X_corr=f_selection\n",
    "    nb_features=len(X_corr[0])\n",
    "\n",
    "    Scores=[]\n",
    "    kf=KFold(n_splits=5,shuffle=False)\n",
    "\n",
    "    for train,test in kf.split(X_corr):\n",
    "        \n",
    "        X1_train_corr=X_corr[train]\n",
    "        X1_test_corr=X_corr[test]\n",
    "        Y1_train_corr=Y[train]\n",
    "        Y1_test_corr=Y[test]\n",
    "\n",
    "        regr = linear_model.LinearRegression()\n",
    "        regr.fit(X1_train_corr, Y1_train_corr)\n",
    "        Y1_pred = regr.predict(X1_test_corr)\n",
    "    \n",
    "        Scores.append(scoreregression(Y1_test_corr,Y1_pred))\n",
    "\n",
    "    print(np.mean(Scores),\"nb_feat:\",nb_features,sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.47083645996900786 nb_feat: 10\n",
      "0.4789502971146858 nb_feat: 12\n",
      "0.48017256227657007 nb_feat: 15\n",
      "0.47913325934797085 nb_feat: 17\n",
      "0.4802395388522539 nb_feat: 20\n"
     ]
    }
   ],
   "source": [
    "for i in X_pca:   #here you choose which kind of feature selection to use(X_correlation,X_pca,X_mutual_info)\n",
    "    linear_regression(X1_val,Y1_val,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_regression(X,Y,f_selection,nb_neigh):\n",
    "    X_knn=f_selection\n",
    "    nb_features=len(X_knn[0])\n",
    "\n",
    "    Scores=[]\n",
    "    kf=KFold(n_splits=5,shuffle=False)\n",
    "\n",
    "    for train,test in kf.split(X_knn):\n",
    "        \n",
    "        X1_train_knn=X_knn[train]\n",
    "        X1_test_knn=X_knn[test]\n",
    "        Y1_train_knn=Y[train]\n",
    "        Y1_test_knn=Y[test]\n",
    "\n",
    "        knn = KNeighborsRegressor(nb_neigh)\n",
    "        knn.fit(X1_train_knn, Y1_train_knn)\n",
    "        Y1_pred = knn.predict(X1_test_knn)\n",
    "\n",
    "        Scores.append(scoreregression(Y1_test_knn,Y1_pred))\n",
    "\n",
    "    print(np.mean(Scores),\"nb_feat:\",nb_features,\"nb_neighb:\",nb_neigh,sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4898504259787968 nb_feat: 10 nb_neighb: 8\n",
      "0.4897838989884164 nb_feat: 12 nb_neighb: 8\n",
      "0.4897906052521808 nb_feat: 15 nb_neighb: 8\n",
      "0.49084036256620733 nb_feat: 17 nb_neighb: 8\n",
      "0.49248333901698105 nb_feat: 20 nb_neighb: 8\n",
      "0.4904811840179929 nb_feat: 10 nb_neighb: 9\n",
      "0.49046912639945733 nb_feat: 12 nb_neighb: 9\n",
      "0.490456587981693 nb_feat: 15 nb_neighb: 9\n",
      "0.4915104504520601 nb_feat: 17 nb_neighb: 9\n",
      "0.490476892820009 nb_feat: 20 nb_neighb: 9\n",
      "0.49250944258270035 nb_feat: 10 nb_neighb: 10\n",
      "0.492483760430739 nb_feat: 12 nb_neighb: 10\n",
      "0.4925087612205809 nb_feat: 15 nb_neighb: 10\n",
      "0.4927630749161288 nb_feat: 17 nb_neighb: 10\n",
      "0.4915545517954035 nb_feat: 20 nb_neighb: 10\n",
      "0.4949688504329637 nb_feat: 10 nb_neighb: 11\n",
      "0.4949450311879384 nb_feat: 12 nb_neighb: 11\n",
      "0.4949699282658601 nb_feat: 15 nb_neighb: 11\n",
      "0.494601224781584 nb_feat: 17 nb_neighb: 11\n",
      "0.4907004936599133 nb_feat: 20 nb_neighb: 11\n",
      "0.49158510095814306 nb_feat: 10 nb_neighb: 12\n",
      "0.4915637444976128 nb_feat: 12 nb_neighb: 12\n",
      "0.49157611827726166 nb_feat: 15 nb_neighb: 12\n",
      "0.4919542223765589 nb_feat: 17 nb_neighb: 12\n",
      "0.49372375173684935 nb_feat: 20 nb_neighb: 12\n",
      "0.4939992579243741 nb_feat: 10 nb_neighb: 13\n",
      "0.493999335257436 nb_feat: 12 nb_neighb: 13\n",
      "0.4939870540782588 nb_feat: 15 nb_neighb: 13\n",
      "0.49433176404567664 nb_feat: 17 nb_neighb: 13\n",
      "0.4934787022520409 nb_feat: 20 nb_neighb: 13\n",
      "0.4946616359298429 nb_feat: 10 nb_neighb: 14\n",
      "0.49465568328666765 nb_feat: 12 nb_neighb: 14\n",
      "0.49465568328666765 nb_feat: 15 nb_neighb: 14\n",
      "0.49484042552896595 nb_feat: 17 nb_neighb: 14\n",
      "0.49486011862577184 nb_feat: 20 nb_neighb: 14\n",
      "0.4947906078380795 nb_feat: 10 nb_neighb: 15\n",
      "0.4947307991084669 nb_feat: 12 nb_neighb: 15\n",
      "0.49474087493491903 nb_feat: 15 nb_neighb: 15\n",
      "0.4938955773497087 nb_feat: 17 nb_neighb: 15\n",
      "0.4958021199879946 nb_feat: 20 nb_neighb: 15\n",
      "0.493974903817296 nb_feat: 10 nb_neighb: 16\n",
      "0.49397838149850326 nb_feat: 12 nb_neighb: 16\n",
      "0.49397254898155907 nb_feat: 15 nb_neighb: 16\n",
      "0.49392483420624594 nb_feat: 17 nb_neighb: 16\n",
      "0.4932069099499838 nb_feat: 20 nb_neighb: 16\n",
      "0.49326006258094085 nb_feat: 10 nb_neighb: 17\n",
      "0.493316988538294 nb_feat: 12 nb_neighb: 17\n",
      "0.4933108689766656 nb_feat: 15 nb_neighb: 17\n",
      "0.49332078850301925 nb_feat: 17 nb_neighb: 17\n",
      "0.4938976737885639 nb_feat: 20 nb_neighb: 17\n",
      "0.4925370819182485 nb_feat: 10 nb_neighb: 18\n",
      "0.49254773856144735 nb_feat: 12 nb_neighb: 18\n",
      "0.4925413766453395 nb_feat: 15 nb_neighb: 18\n",
      "0.4936239412880674 nb_feat: 17 nb_neighb: 18\n",
      "0.4943562215483862 nb_feat: 20 nb_neighb: 18\n",
      "0.4924405047038209 nb_feat: 10 nb_neighb: 19\n",
      "0.4924645963069068 nb_feat: 12 nb_neighb: 19\n",
      "0.4924645963069068 nb_feat: 15 nb_neighb: 19\n",
      "0.4927817612502796 nb_feat: 17 nb_neighb: 19\n",
      "0.49274550089475894 nb_feat: 20 nb_neighb: 19\n",
      "0.49380192625623565 nb_feat: 10 nb_neighb: 20\n",
      "0.49379962521022513 nb_feat: 12 nb_neighb: 20\n",
      "0.49381144976535907 nb_feat: 15 nb_neighb: 20\n",
      "0.49246298533962196 nb_feat: 17 nb_neighb: 20\n",
      "0.4921525243032764 nb_feat: 20 nb_neighb: 20\n"
     ]
    }
   ],
   "source": [
    "neighbours=[8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "\n",
    "for i in neighbours:\n",
    "    for j in X_mutual_info:   #here you choose which kind of feature selection to use(X_correlation,X_pca,X_mutual_info)\n",
    "        knn_regression(X1_val,Y1_val,j,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_regression(X,Y,f_selection,layers,learning_r):\n",
    "    X_mlp=f_selection\n",
    "    nb_features=len(X_mlp[0])\n",
    "\n",
    "    Scores=[]\n",
    "    kf=KFold(n_splits=5,shuffle=False)\n",
    "\n",
    "    for train,test in kf.split(X_mlp):\n",
    "        \n",
    "        X1_train_pca=X_mlp[train]\n",
    "        X1_test_pca=X_mlp[test]\n",
    "        Y1_train_pca=Y[train]\n",
    "        Y1_test_pca=Y[test]\n",
    "\n",
    "        reg = MLPRegressor(hidden_layer_sizes=layers,activation=\"relu\",learning_rate=learning_r,max_iter=200)\n",
    "        reg.fit(X1_train_pca, Y1_train_pca)\n",
    "        Y1_pred=reg.predict(X1_test_pca)\n",
    "\n",
    "        Scores.append(scoreregression(Y1_test_pca,Y1_pred))\n",
    "\n",
    "    print(np.mean(Scores),\"nb_feat:\",nb_features,\"layers:\",layers,\"lear_rate:\",learning_r,sep=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4860165044717406 nb_feat: 10 layers: 10 lear_rate: constant\n",
      "0.4859692997659225 nb_feat: 10 layers: 10 lear_rate: invscaling\n",
      "0.48080533050416535 nb_feat: 10 layers: 10 lear_rate: adaptive\n",
      "0.48116962135658026 nb_feat: 12 layers: 10 lear_rate: constant\n",
      "0.48796766994431745 nb_feat: 12 layers: 10 lear_rate: invscaling\n",
      "0.48696602920921617 nb_feat: 12 layers: 10 lear_rate: adaptive\n"
     ]
    }
   ],
   "source": [
    "layers=[(10),(12),(15),(10,10),(12,12),(15,15)]\n",
    "learning_r = [\"constant\",\"invscaling\",\"adaptive\"]\n",
    "\n",
    "for i in layers:\n",
    "    for j in X_mutual_info: #here you choose which kind of feature selection to use(X_correlation,X_pca,X_mutual_info)\n",
    "        for k in learning_r:\n",
    "            mlp_regression(X1_val,Y1_val,j,i,k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\nscaler_bis = StandardScaler(copy=True,with_mean=True,with_std=True)\\nX1_normalized = scaler_bis.fit_transform(X1_val)\\n\\npca_bis = PCA(n_components=X1.shape[-1])\\n\\ndata_transformed = pca_bis.fit_transform(X1_normalized)\\neig_val = pca_bis.explained_variance_\\neig_vec = pca_bis.components_\\n\\n# Compute an array E, where E(P) indicates the variance captured in the first P component.\\nE = np.array([eig_val[:p+1].sum()/eig_val.sum() for p in range(len(eig_val))])\\ntau = 0.95 # Threshold\\n\\n# Find the minimum P that captures \\tau portion of the variance\\nP = np.where(E>tau)[0][0] +1\\n\\nprint('Minimum number of components that preserve {} of the variance = {} \\n' .format(tau,P))\\nfig = plt.figure(figsize=(20,5))\\nax1 = fig.add_subplot() #(131)\\nax1.plot(np.arange(1,E.shape[0]+1), E, 'o-', markersize=8, color='blue', alpha=0.5)\\nax1.set_xlabel('number of components')\\nax1.set_ylabel('Preserved variance')\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 16
    }
   ],
   "source": [
    "\"\"\"\n",
    "scaler_bis = StandardScaler(copy=True,with_mean=True,with_std=True)\n",
    "X1_normalized = scaler_bis.fit_transform(X1_val)\n",
    "\n",
    "pca_bis = PCA(n_components=X1.shape[-1])\n",
    "\n",
    "data_transformed = pca_bis.fit_transform(X1_normalized)\n",
    "eig_val = pca_bis.explained_variance_\n",
    "eig_vec = pca_bis.components_\n",
    "\n",
    "# Compute an array E, where E(P) indicates the variance captured in the first P component.\n",
    "E = np.array([eig_val[:p+1].sum()/eig_val.sum() for p in range(len(eig_val))])\n",
    "tau = 0.95 # Threshold\n",
    "\n",
    "# Find the minimum P that captures \\tau portion of the variance\n",
    "P = np.where(E>tau)[0][0] +1\n",
    "\n",
    "print('Minimum number of components that preserve {} of the variance = {} \\n' .format(tau,P))\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "ax1 = fig.add_subplot() #(131)\n",
    "ax1.plot(np.arange(1,E.shape[0]+1), E, 'o-', markersize=8, color='blue', alpha=0.5)\n",
    "ax1.set_xlabel('number of components')\n",
    "ax1.set_ylabel('Preserved variance')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nparameters={\\n\\'learning_rate\\': [\"adaptive\"],\\n\\'hidden_layer_sizes\\':[(20),(30),(35),(20,20),(30,30),(35,35)],\\n\\'alpha\\': [0.001,0.0001,0.00001],\\n\\'activation\\': [\"relu\"]\\n}\\n\\nmlp=MLPRegressor()\\n\\nclf = GridSearchCV(mlp, parameters)\\n\\nclf.fit(X1_train_pca, Y1_train)\\n\\nprint(clf.best_params_)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 17
    }
   ],
   "source": [
    "\"\"\"\n",
    "parameters={\n",
    "'learning_rate': [\"adaptive\"],\n",
    "'hidden_layer_sizes':[(20),(30),(35),(20,20),(30,30),(35,35)],\n",
    "'alpha': [0.001,0.0001,0.00001],\n",
    "'activation': [\"relu\"]\n",
    "}\n",
    "\n",
    "mlp=MLPRegressor()\n",
    "\n",
    "clf = GridSearchCV(mlp, parameters)\n",
    "\n",
    "clf.fit(X1_train_pca, Y1_train)\n",
    "\n",
    "print(clf.best_params_)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}