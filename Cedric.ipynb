{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import zscore\n",
    "\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    use_seaborn = True\n",
    "    sns.set()\n",
    "except:\n",
    "    use_seaborn = False\n",
    "\n",
    "showfig = False\n",
    "delOutliers = False\n",
    "downSizing  = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data \n",
    "X1 = pd.read_csv(\"X1.csv\")\n",
    "Y1 = pd.read_csv(\"Y1.csv\",header=None,names =['shares'])\n",
    "\n",
    "if showfig:\n",
    "    fig = plt.figure(figsize=(6.4*2,4.8*6))\n",
    "    gs  = fig.add_gridspec(nrows=12, ncols=5)\n",
    "    for (i,header) in enumerate(X1.columns):\n",
    "        ax = fig.add_subplot(gs[int(i/5),i%5])\n",
    "        ax.scatter(X1[header],Y1.values, s=5)\n",
    "        ax.set_xlabel(header)\n",
    "    fig.tight_layout()\n",
    "\n",
    "X1_val = X1.values\n",
    "Y1_val = Y1.values\n",
    "\n",
    "X1_train, X1_test, Y1_train, Y1_test = train_test_split(X1_val, Y1_val,random_state=1, test_size=0.2)\n",
    "\n",
    "X1_val = X1_train\n",
    "Y1_val = Y1_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if delOutliers:\n",
    "#Removing outliers\n",
    "    z_scores = zscore(X1_val)\n",
    "    abs_z_scores = np.abs(z_scores)\n",
    "    X1_filtered_ind = (abs_z_scores < 4).all(axis=1)\n",
    "    X1_val = X1_val[X1_filtered_ind]\n",
    "    Y1_val = Y1_val[X1_filtered_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if downSizing:\n",
    "    # corr = np.corrcoef(X1_train)\n",
    "    # corr = np.triu(corr) - np.eye(corr.shape[0]) #np.diag(np.diag(corr))\n",
    "    # delt = np.where(corr>1-1e-7)\n",
    "\n",
    "    # keep = np.where([idx not in delt[0] for idx in range(corr.shape[0])])[0]\n",
    "    # print(X1_val.shape[0], len(keep))\n",
    "\n",
    "    # X1_train,Y1_train = X1_train[keep,:],Y1_train[keep,:]\n",
    "    \n",
    "    corr = np.corrcoef(X1_val.transpose())\n",
    "    corr = np.triu(corr) - np.eye(corr.shape[0]) #np.diag(np.diag(corr))\n",
    "    delt = np.where(corr>1-1e-1)\n",
    "\n",
    "    keep = np.where([idx not in delt[0] for idx in range(corr.shape[0])])[0]\n",
    "    print(X1_val.shape[1], len(keep))\n",
    "\n",
    "    X1_val = X1_val[:,keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score computation\n",
    "def scoref1(ytrue, ypred, th):\n",
    "    return sklearn.metrics.f1_score(ytrue>th, ypred>th)\n",
    "\n",
    "def scoreregression(ytrue, ypred):\n",
    "    scores = [\n",
    "        scoref1(ytrue, ypred, th=th) for th in [ 500, 1400, 5000, 10000]\n",
    "    ]\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selections\n",
    "def correlation_selection(X,Y,nb):\n",
    "    data = np.concatenate((np.transpose(X),np.transpose(Y)))\n",
    "    corr = np.corrcoef(data)[:,-1]\n",
    "    corr_bis=corr[:-1]\n",
    "    idxs = np.argpartition(corr_bis, -nb)[-nb:]\n",
    "    X_corr = X[:,idxs]\n",
    "    return X_corr\n",
    "\n",
    "def PCA_selection(X,nb):\n",
    "    # To do 1: initiate StandardScaler class\n",
    "    scaler = StandardScaler(copy=True,with_mean=True,with_std=True)\n",
    "    # Initialize PCA\n",
    "    pca = PCA(n_components=nb)\n",
    "    X_pca = pca.fit_transform(scaler.fit_transform(X))\n",
    "    \n",
    "    return X_pca\n",
    "\n",
    "def mutual_info_selection(X,Y,nb):\n",
    "    mutual_information = mutual_info_regression(X,Y)\n",
    "    idxs = np.argpartition(mutual_information,-nb)[-nb:]\n",
    "    X_mutual_info = X[:,idxs[:]]\n",
    "\n",
    "    return X_mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_correlation_10=correlation_selection(X1_val,Y1_val,10)\n",
    "X_correlation_12=correlation_selection(X1_val,Y1_val,12)\n",
    "X_correlation_15=correlation_selection(X1_val,Y1_val,15)\n",
    "X_correlation_17=correlation_selection(X1_val,Y1_val,17)\n",
    "X_correlation_20=correlation_selection(X1_val,Y1_val,20)\n",
    "\n",
    "X_correlation=[X_correlation_10,X_correlation_12,X_correlation_15,X_correlation_17,X_correlation_20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pca_10=PCA_selection(X1_val,10)\n",
    "X_pca_12=PCA_selection(X1_val,12)\n",
    "X_pca_15=PCA_selection(X1_val,15)\n",
    "X_pca_17=PCA_selection(X1_val,17)\n",
    "X_pca_20=PCA_selection(X1_val,20)\n",
    "\n",
    "X_pca=[X_pca_10,X_pca_12,X_pca_15,X_pca_17,X_pca_20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nX_mutual_info_10=mutual_info_selection(X1_val,Y1_val,10)\\nX_mutual_info_12=mutual_info_selection(X1_val,Y1_val,12)\\nX_mutual_info_15=mutual_info_selection(X1_val,Y1_val,15)\\nX_mutual_info_17=mutual_info_selection(X1_val,Y1_val,17)\\nX_mutual_info_20=mutual_info_selection(X1_val,Y1_val,20)\\n\\nX_mutual_info=[X_mutual_info_10,X_mutual_info_12,X_mutual_info_15,X_mutual_info_17,X_mutual_info_20]\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ],
   "source": [
    "\"\"\"\n",
    "X_mutual_info_10=mutual_info_selection(X1_val,Y1_val,10)\n",
    "X_mutual_info_12=mutual_info_selection(X1_val,Y1_val,12)\n",
    "X_mutual_info_15=mutual_info_selection(X1_val,Y1_val,15)\n",
    "X_mutual_info_17=mutual_info_selection(X1_val,Y1_val,17)\n",
    "X_mutual_info_20=mutual_info_selection(X1_val,Y1_val,20)\n",
    "\n",
    "X_mutual_info=[X_mutual_info_10,X_mutual_info_12,X_mutual_info_15,X_mutual_info_17,X_mutual_info_20]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X,Y,f_selection):\n",
    "    X_corr=f_selection\n",
    "    nb_features=len(X_corr[0])\n",
    "\n",
    "    Scores=[]\n",
    "    kf=KFold(n_splits=5,shuffle=False)\n",
    "\n",
    "    for train,test in kf.split(X_corr):\n",
    "        \n",
    "        X1_train_corr=X_corr[train]\n",
    "        X1_test_corr=X_corr[test]\n",
    "        Y1_train_corr=Y[train]\n",
    "        Y1_test_corr=Y[test]\n",
    "\n",
    "        regr = linear_model.LinearRegression()\n",
    "        regr.fit(X1_train_corr, Y1_train_corr)\n",
    "        Y1_pred = regr.predict(X1_test_corr)\n",
    "    \n",
    "        Scores.append(scoreregression(Y1_test_corr,Y1_pred))\n",
    "\n",
    "    print(np.mean(Scores),\"nb_feat:\",nb_features,sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4652507987002097 nb_feat: 10\n",
      "0.4737156732717459 nb_feat: 12\n",
      "0.47639723713135884 nb_feat: 15\n",
      "0.4764154336145602 nb_feat: 17\n",
      "0.47705964952393537 nb_feat: 20\n"
     ]
    }
   ],
   "source": [
    "for i in X_pca:   #here you choose which kind of feature selection to use(X_correlation,X_pca,X_mutual_info)\n",
    "    linear_regression(X1_val,Y1_val,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_regression(X,Y,f_selection,nb_neigh):\n",
    "    X_knn=f_selection\n",
    "    nb_features=len(X_knn[0])\n",
    "\n",
    "    Scores=[]\n",
    "    kf=KFold(n_splits=5,shuffle=False)\n",
    "\n",
    "    for train,test in kf.split(X_knn):\n",
    "        \n",
    "        X1_train_knn=X_knn[train]\n",
    "        X1_test_knn=X_knn[test]\n",
    "        Y1_train_knn=Y[train]\n",
    "        Y1_test_knn=Y[test]\n",
    "\n",
    "        knn = KNeighborsRegressor(nb_neigh)\n",
    "        knn.fit(X1_train_knn, Y1_train_knn)\n",
    "        Y1_pred = knn.predict(X1_test_knn)\n",
    "\n",
    "        Scores.append(scoreregression(Y1_test_knn,Y1_pred))\n",
    "\n",
    "    print(np.mean(Scores),\"nb_feat:\",nb_features,\"nb_neighb:\",nb_neigh,sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.48335588127574225 nb_feat: 10 nb_neighb: 8\n",
      "0.48984924677501496 nb_feat: 12 nb_neighb: 8\n",
      "0.48461319909121003 nb_feat: 15 nb_neighb: 8\n",
      "0.4877117005951764 nb_feat: 17 nb_neighb: 8\n",
      "0.48070895955858095 nb_feat: 20 nb_neighb: 8\n",
      "0.4861606948633911 nb_feat: 10 nb_neighb: 9\n",
      "0.4884196299588255 nb_feat: 12 nb_neighb: 9\n",
      "0.4878562394146158 nb_feat: 15 nb_neighb: 9\n",
      "0.4902076009284463 nb_feat: 17 nb_neighb: 9\n",
      "0.48222786350599806 nb_feat: 20 nb_neighb: 9\n",
      "0.48514350959039537 nb_feat: 10 nb_neighb: 10\n",
      "0.48948343019356216 nb_feat: 12 nb_neighb: 10\n",
      "0.4882160028618171 nb_feat: 15 nb_neighb: 10\n",
      "0.4870568528294121 nb_feat: 17 nb_neighb: 10\n",
      "0.4833477761785992 nb_feat: 20 nb_neighb: 10\n",
      "0.48539212325948533 nb_feat: 10 nb_neighb: 11\n",
      "0.4884761863433873 nb_feat: 12 nb_neighb: 11\n",
      "0.48722560507646495 nb_feat: 15 nb_neighb: 11\n",
      "0.4872793724932659 nb_feat: 17 nb_neighb: 11\n",
      "0.48138073618942656 nb_feat: 20 nb_neighb: 11\n",
      "0.48447196850264895 nb_feat: 10 nb_neighb: 12\n",
      "0.48695248694505117 nb_feat: 12 nb_neighb: 12\n",
      "0.4862041537239672 nb_feat: 15 nb_neighb: 12\n",
      "0.48943500722205435 nb_feat: 17 nb_neighb: 12\n",
      "0.4819602860028378 nb_feat: 20 nb_neighb: 12\n",
      "0.48360703655096515 nb_feat: 10 nb_neighb: 13\n",
      "0.48909318156155085 nb_feat: 12 nb_neighb: 13\n",
      "0.4895779034506342 nb_feat: 15 nb_neighb: 13\n",
      "0.48867430894912556 nb_feat: 17 nb_neighb: 13\n",
      "0.48327171594529217 nb_feat: 20 nb_neighb: 13\n",
      "0.4823884687172284 nb_feat: 10 nb_neighb: 14\n",
      "0.4911370898750961 nb_feat: 12 nb_neighb: 14\n",
      "0.4924654962104677 nb_feat: 15 nb_neighb: 14\n",
      "0.48948946116010583 nb_feat: 17 nb_neighb: 14\n",
      "0.4852503139676688 nb_feat: 20 nb_neighb: 14\n",
      "0.48461469251723177 nb_feat: 10 nb_neighb: 15\n",
      "0.49139725725190486 nb_feat: 12 nb_neighb: 15\n",
      "0.49063619636856587 nb_feat: 15 nb_neighb: 15\n",
      "0.4883331508771893 nb_feat: 17 nb_neighb: 15\n",
      "0.48100560809691795 nb_feat: 20 nb_neighb: 15\n",
      "0.4834069425373568 nb_feat: 10 nb_neighb: 16\n",
      "0.49087227884422735 nb_feat: 12 nb_neighb: 16\n",
      "0.491507336337325 nb_feat: 15 nb_neighb: 16\n",
      "0.4901659993391888 nb_feat: 17 nb_neighb: 16\n",
      "0.4817310215105774 nb_feat: 20 nb_neighb: 16\n",
      "0.4844875431954104 nb_feat: 10 nb_neighb: 17\n",
      "0.4894670606871932 nb_feat: 12 nb_neighb: 17\n",
      "0.49294445466968206 nb_feat: 15 nb_neighb: 17\n",
      "0.4892940989705104 nb_feat: 17 nb_neighb: 17\n",
      "0.480435133490445 nb_feat: 20 nb_neighb: 17\n",
      "0.4857467636973146 nb_feat: 10 nb_neighb: 18\n",
      "0.4896583309015594 nb_feat: 12 nb_neighb: 18\n",
      "0.4919996022326849 nb_feat: 15 nb_neighb: 18\n",
      "0.48679414543521016 nb_feat: 17 nb_neighb: 18\n",
      "0.4791105693570552 nb_feat: 20 nb_neighb: 18\n",
      "0.48516665161041717 nb_feat: 10 nb_neighb: 19\n",
      "0.48835692624848914 nb_feat: 12 nb_neighb: 19\n",
      "0.49299701922758093 nb_feat: 15 nb_neighb: 19\n",
      "0.4872415058122021 nb_feat: 17 nb_neighb: 19\n",
      "0.4806416219827951 nb_feat: 20 nb_neighb: 19\n",
      "0.48195457617989107 nb_feat: 10 nb_neighb: 20\n",
      "0.488793814694657 nb_feat: 12 nb_neighb: 20\n",
      "0.49170193480279367 nb_feat: 15 nb_neighb: 20\n",
      "0.4880659736339183 nb_feat: 17 nb_neighb: 20\n",
      "0.48084258772957905 nb_feat: 20 nb_neighb: 20\n"
     ]
    }
   ],
   "source": [
    "\n",
    "neighbours=[8,9,10,11,12,13,14,15,16,17,18,19,20]\n",
    "\n",
    "for i in neighbours:\n",
    "    for j in X_pca:   #here you choose which kind of feature selection to use(X_correlation,X_pca,X_mutual_info)\n",
    "        knn_regression(X1_val,Y1_val,j,i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_regression(X,Y,f_selection,layers,learning_r):\n",
    "    X_mlp=f_selection\n",
    "    nb_features=len(X_mlp[0])\n",
    "\n",
    "    Scores=[]\n",
    "    kf=KFold(n_splits=5,shuffle=False)\n",
    "\n",
    "    for train,test in kf.split(X_mlp):\n",
    "        \n",
    "        X1_train_mlp=X_mlp[train]\n",
    "        X1_test_mlp=X_mlp[test]\n",
    "        Y1_train_mlp=Y[train]\n",
    "        Y1_test_mlp=Y[test]\n",
    "\n",
    "        reg = MLPRegressor(hidden_layer_sizes=layers,activation=\"relu\",learning_rate=learning_r,max_iter=200)\n",
    "        reg.fit(X1_train_mlp, Y1_train_mlp)\n",
    "        Y1_pred=reg.predict(X1_test_mlp)\n",
    "\n",
    "        Scores.append(scoreregression(Y1_test_mlp,Y1_pred))\n",
    "\n",
    "    print(np.mean(Scores),\"nb_feat:\",nb_features,\"layers:\",layers,\"lear_rate:\",learning_r,sep=\" \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nlayers=[(10),(12),(15),(10,10),(12,12),(15,15)]\\nlearning_r = [\"constant\",\"invscaling\",\"adaptive\"]\\n\\nfor i in layers:\\n    for j in X_mutual_info: #here you choose which kind of feature selection to use(X_correlation,X_pca,X_mutual_info)\\n        for k in learning_r:\\n            mlp_regression(X1_val,Y1_val,j,i,k)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 15
    }
   ],
   "source": [
    "\"\"\"\n",
    "layers=[(10),(12),(15),(10,10),(12,12),(15,15)]\n",
    "learning_r = [\"constant\",\"invscaling\",\"adaptive\"]\n",
    "\n",
    "for i in layers:\n",
    "    for j in X_mutual_info: #here you choose which kind of feature selection to use(X_correlation,X_pca,X_mutual_info)\n",
    "        for k in learning_r:\n",
    "            mlp_regression(X1_val,Y1_val,j,i,k)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pls_regression(X,Y,f_selection,nb_comp):\n",
    "    X_pls=f_selection\n",
    "    nb_features=len(X_pls[0])\n",
    "\n",
    "    Scores=[]\n",
    "    kf=KFold(n_splits=5,shuffle=False)\n",
    "\n",
    "    for train,test in kf.split(X_pls):\n",
    "        \n",
    "        X1_train_pls=X_pls[train]\n",
    "        X1_test_pls=X_pls[test]\n",
    "        Y1_train_pls=Y[train]\n",
    "        Y1_test_pls=Y[test]\n",
    "\n",
    "        pls = PLSRegression(n_components=nb_comp,max_iter=200)\n",
    "        pls.fit(X1_train_pls, Y1_train_pls)\n",
    "        Y1_pred=pls.predict(X1_test_pls)\n",
    "\n",
    "        Scores.append(scoreregression(Y1_test_pls,Y1_pred))\n",
    "\n",
    "    print(np.mean(Scores),\"nb_feat:\",nb_features,\"nb_comp:\",nb_comp,sep=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4646835782297223 nb_feat: 10 nb_comp: 1\n",
      "0.47404944383573966 nb_feat: 12 nb_comp: 1\n",
      "0.4757976771001632 nb_feat: 15 nb_comp: 1\n",
      "0.4756746535682469 nb_feat: 17 nb_comp: 1\n",
      "0.4769720675498269 nb_feat: 20 nb_comp: 1\n",
      "0.4654788523078771 nb_feat: 10 nb_comp: 2\n",
      "0.473859414642248 nb_feat: 12 nb_comp: 2\n",
      "0.4762849353150128 nb_feat: 15 nb_comp: 2\n",
      "0.4764706069560436 nb_feat: 17 nb_comp: 2\n",
      "0.4770223421989496 nb_feat: 20 nb_comp: 2\n",
      "0.4652507987002097 nb_feat: 10 nb_comp: 4\n",
      "0.4737156732717459 nb_feat: 12 nb_comp: 4\n",
      "0.47639723713135884 nb_feat: 15 nb_comp: 4\n",
      "0.4764154336145602 nb_feat: 17 nb_comp: 4\n",
      "0.47705964952393537 nb_feat: 20 nb_comp: 4\n",
      "0.4652507987002097 nb_feat: 10 nb_comp: 6\n",
      "0.4737156732717459 nb_feat: 12 nb_comp: 6\n",
      "0.47639723713135884 nb_feat: 15 nb_comp: 6\n",
      "0.4764154336145602 nb_feat: 17 nb_comp: 6\n",
      "0.47705964952393537 nb_feat: 20 nb_comp: 6\n",
      "0.4652507987002097 nb_feat: 10 nb_comp: 8\n",
      "0.4737156732717459 nb_feat: 12 nb_comp: 8\n",
      "0.47639723713135884 nb_feat: 15 nb_comp: 8\n",
      "0.4764154336145602 nb_feat: 17 nb_comp: 8\n",
      "0.47705964952393537 nb_feat: 20 nb_comp: 8\n",
      "0.4652507987002097 nb_feat: 10 nb_comp: 10\n",
      "0.4737156732717459 nb_feat: 12 nb_comp: 10\n",
      "0.47639723713135884 nb_feat: 15 nb_comp: 10\n",
      "0.4764154336145602 nb_feat: 17 nb_comp: 10\n",
      "0.47705964952393537 nb_feat: 20 nb_comp: 10\n"
     ]
    }
   ],
   "source": [
    "nb_comp=[1,2,4,6,8,10]\n",
    "\n",
    "for i in nb_comp:\n",
    "    for j in X_pca:   #here you choose which kind of feature selection to use(X_correlation,X_pca,X_mutual_info)\n",
    "        pls_regression(X1_val,Y1_val,j,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\nscaler_bis = StandardScaler(copy=True,with_mean=True,with_std=True)\\nX1_normalized = scaler_bis.fit_transform(X1_val)\\n\\npca_bis = PCA(n_components=X1.shape[-1])\\n\\ndata_transformed = pca_bis.fit_transform(X1_normalized)\\neig_val = pca_bis.explained_variance_\\neig_vec = pca_bis.components_\\n\\n# Compute an array E, where E(P) indicates the variance captured in the first P component.\\nE = np.array([eig_val[:p+1].sum()/eig_val.sum() for p in range(len(eig_val))])\\ntau = 0.95 # Threshold\\n\\n# Find the minimum P that captures \\tau portion of the variance\\nP = np.where(E>tau)[0][0] +1\\n\\nprint('Minimum number of components that preserve {} of the variance = {} \\n' .format(tau,P))\\nfig = plt.figure(figsize=(20,5))\\nax1 = fig.add_subplot() #(131)\\nax1.plot(np.arange(1,E.shape[0]+1), E, 'o-', markersize=8, color='blue', alpha=0.5)\\nax1.set_xlabel('number of components')\\nax1.set_ylabel('Preserved variance')\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 20
    }
   ],
   "source": [
    "\"\"\"\n",
    "scaler_bis = StandardScaler(copy=True,with_mean=True,with_std=True)\n",
    "X1_normalized = scaler_bis.fit_transform(X1_val)\n",
    "\n",
    "pca_bis = PCA(n_components=X1.shape[-1])\n",
    "\n",
    "data_transformed = pca_bis.fit_transform(X1_normalized)\n",
    "eig_val = pca_bis.explained_variance_\n",
    "eig_vec = pca_bis.components_\n",
    "\n",
    "# Compute an array E, where E(P) indicates the variance captured in the first P component.\n",
    "E = np.array([eig_val[:p+1].sum()/eig_val.sum() for p in range(len(eig_val))])\n",
    "tau = 0.95 # Threshold\n",
    "\n",
    "# Find the minimum P that captures \\tau portion of the variance\n",
    "P = np.where(E>tau)[0][0] +1\n",
    "\n",
    "print('Minimum number of components that preserve {} of the variance = {} \\n' .format(tau,P))\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "ax1 = fig.add_subplot() #(131)\n",
    "ax1.plot(np.arange(1,E.shape[0]+1), E, 'o-', markersize=8, color='blue', alpha=0.5)\n",
    "ax1.set_xlabel('number of components')\n",
    "ax1.set_ylabel('Preserved variance')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nparameters={\\n\\'learning_rate\\': [\"adaptive\"],\\n\\'hidden_layer_sizes\\':[(20),(30),(35),(20,20),(30,30),(35,35)],\\n\\'alpha\\': [0.001,0.0001,0.00001],\\n\\'activation\\': [\"relu\"]\\n}\\n\\nmlp=MLPRegressor()\\n\\nclf = GridSearchCV(mlp, parameters)\\n\\nclf.fit(X1_train_pca, Y1_train)\\n\\nprint(clf.best_params_)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 21
    }
   ],
   "source": [
    "\"\"\"\n",
    "parameters={\n",
    "'learning_rate': [\"adaptive\"],\n",
    "'hidden_layer_sizes':[(20),(30),(35),(20,20),(30,30),(35,35)],\n",
    "'alpha': [0.001,0.0001,0.00001],\n",
    "'activation': [\"relu\"]\n",
    "}\n",
    "\n",
    "mlp=MLPRegressor()\n",
    "\n",
    "clf = GridSearchCV(mlp, parameters)\n",
    "\n",
    "clf.fit(X1_train_pca, Y1_train)\n",
    "\n",
    "print(clf.best_params_)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}