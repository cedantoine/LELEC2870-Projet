{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import scipy\n",
    "from scipy.stats import zscore\n",
    "\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    use_seaborn = True\n",
    "    sns.set()\n",
    "except:\n",
    "    use_seaborn = False\n",
    "\n",
    "showfig = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data \n",
    "X1 = pd.read_csv(\"X1.csv\")\n",
    "Y1 = pd.read_csv(\"Y1.csv\",header=None,names =['shares'])\n",
    "\n",
    "if showfig:\n",
    "    fig = plt.figure(figsize=(6.4*2,4.8*6))\n",
    "    gs  = fig.add_gridspec(nrows=12, ncols=5)\n",
    "    for (i,header) in enumerate(X1.columns):\n",
    "        ax = fig.add_subplot(gs[int(i/5),i%5])\n",
    "        ax.scatter(X1[header],Y1.values, s=5)\n",
    "        ax.set_xlabel(header)\n",
    "    fig.tight_layout()\n",
    "\n",
    "X1_val = X1.values\n",
    "Y1_val = Y1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Removing outliers\n",
    "z_scores = zscore(X1_val)\n",
    "abs_z_scores = np.abs(z_scores)\n",
    "X1_filtered_ind = (abs_z_scores < 4).all(axis=1)\n",
    "X1_filtered = X1_val[X1_filtered_ind]\n",
    "Y1_filtered = Y1_val[X1_filtered_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score computation\n",
    "def scoref1(ytrue, ypred, th):\n",
    "    return sklearn.metrics.f1_score(ytrue>th, ypred>th)\n",
    "\n",
    "def scoreregression(ytrue, ypred):\n",
    "    scores = [\n",
    "        scoref1(ytrue, ypred, th=th) for th in [ 500, 1400, 5000, 10000]\n",
    "    ]\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selections\n",
    "def correlation_selection(X,Y,nb):\n",
    "    data = np.concatenate((np.transpose(X),np.transpose(Y)))\n",
    "    corr = np.corrcoef(data)[:-1]\n",
    "    idxs = np.argpartition(corr[:][57], -nb)[-nb:]\n",
    "    X_corr = X[:,idxs]\n",
    "    #ici il y a encore un souci!!!!\n",
    "    return X_corr\n",
    "\n",
    "def PCA_selection(X,nb):\n",
    "    # To do 1: initiate StandardScaler class\n",
    "    scaler = StandardScaler(copy=True,with_mean=True,with_std=True)\n",
    "    # Initialize PCA\n",
    "    pca = PCA(n_components=nb)\n",
    "    X_pca = pca.fit_transform(scaler.fit_transform(X))\n",
    "    \n",
    "    return X_pca\n",
    "\n",
    "def mutual_info_selection(X,Y,nb):\n",
    "    mutual_information = mutual_info_regression(X,Y)\n",
    "    idxs = np.argpartition(mutual_information,-nb)[-nb:]\n",
    "    X_mutual_info = X[:,idxs[:]]\n",
    "\n",
    "    return X_mutual_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_mutual_info=mutual_info_selection(X1_val,Y1_val,17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(19822, 17)"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "np.shape(X_mutual_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X,Y,nb_features):\n",
    "    X_corr=correlation_selection(X,Y,nb_features)\n",
    "    #X_corr=PCA_selection(X,nb_features)\n",
    "    #X_corr=X_mutual_info\n",
    "\n",
    "    Scores=[]\n",
    "    kf=KFold(n_splits=5,shuffle=False)\n",
    "\n",
    "    for train,test in kf.split(X_corr):\n",
    "        \n",
    "        X1_train_corr=X_corr[train]\n",
    "        X1_test_corr=X_corr[test]\n",
    "        Y1_train_corr=Y[train]\n",
    "        Y1_test_corr=Y[test]\n",
    "\n",
    "        regr = linear_model.LinearRegression()\n",
    "        regr.fit(X1_train_corr, Y1_train_corr)\n",
    "        Y1_pred = regr.predict(X1_test_corr)\n",
    "    \n",
    "        Scores.append(scoreregression(Y1_test_corr,Y1_pred))\n",
    "\n",
    "    print(np.mean(Scores),nb_features,sep=\"   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4849599136705819   20\n"
     ]
    }
   ],
   "source": [
    "linear_regression(X1_val,Y1_val,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_regression(X,Y,nb_features):\n",
    "    #X_knn=correlation_selection(X,Y,nb_features)\n",
    "    #X_knn=PCA_selection(X,nb_features)\n",
    "    X_knn=X_mutual_info\n",
    "\n",
    "    Scores=[]\n",
    "    kf=KFold(n_splits=5,shuffle=False)\n",
    "\n",
    "    for train,test in kf.split(X_knn):\n",
    "        \n",
    "        X1_train_pca=X_knn[train]\n",
    "        X1_test_pca=X_knn[test]\n",
    "        Y1_train_pca=Y[train]\n",
    "        Y1_test_pca=Y[test]\n",
    "\n",
    "        knn = KNeighborsRegressor(nb_features)\n",
    "        knn.fit(X1_train_pca, Y1_train_pca)\n",
    "        Y1_pred = knn.predict(X1_test_pca)\n",
    "\n",
    "        Scores.append(scoreregression(Y1_test_pca,Y1_pred))\n",
    "\n",
    "    print(np.mean(Scores),nb_features,sep=\"   \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4724468058999357   3\n",
      "0.4778710804654097   4\n",
      "0.48487126273445424   5\n",
      "0.4874683355818049   6\n",
      "0.4870852501523822   7\n",
      "0.48747334741499204   8\n",
      "0.4875302235782426   9\n",
      "0.4902332738810177   10\n",
      "0.48828877340797383   11\n",
      "0.48881064317901063   12\n",
      "0.48949613516796253   13\n",
      "0.4889513607062398   14\n",
      "0.48734851370808724   15\n",
      "0.4878633868827243   16\n",
      "0.4879243791722162   17\n",
      "0.4870469044371368   18\n",
      "0.48639423559770273   19\n",
      "0.4856509179624937   20\n",
      "0.48739001546220545   21\n",
      "0.48594623815054394   22\n",
      "0.48556652780460957   23\n",
      "0.48510174632537434   24\n"
     ]
    }
   ],
   "source": [
    "neighbours=[3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n",
    "\n",
    "for i in neighbours:\n",
    "    knn_regression(X1_val,Y1_val,i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_regression(X,Y,nb_features,layers):\n",
    "    #X_mlp=correlation_selection(X,Y,nb_features)\n",
    "    #X_mlp=PCA_selection(X,nb_features)\n",
    "    X_mlp=X_mutual_info\n",
    "\n",
    "    Scores=[]\n",
    "    kf=KFold(n_splits=5,shuffle=False)\n",
    "\n",
    "    for train,test in kf.split(X_mlp):\n",
    "        \n",
    "        X1_train_pca=X_mlp[train]\n",
    "        X1_test_pca=X_mlp[test]\n",
    "        Y1_train_pca=Y[train]\n",
    "        Y1_test_pca=Y[test]\n",
    "\n",
    "        reg = MLPRegressor(hidden_layer_sizes=layers,activation=\"relu\" ,random_state=1, max_iter=200)\n",
    "        reg.fit(X1_train_pca, Y1_train_pca)\n",
    "        Y1_pred=reg.predict(X1_test_pca)\n",
    "\n",
    "        Scores.append(scoreregression(Y1_test_pca,Y1_pred))\n",
    "\n",
    "    print(np.mean(Scores),nb_features,layers,sep=\"   \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.48926678621854086   17   15\n"
     ]
    }
   ],
   "source": [
    "mlp_regression(X1_val,Y1_val,17,(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.48926678621854086   17   15\n",
      "0.4884931424260236   17   (15, 15)\n",
      "0.4859887247245722   17   (12, 12)\n",
      "0.489980447363199   17   (13, 13)\n",
      "0.48756894162883385   17   (14, 14)\n",
      "0.49392601059907826   17   (16, 16)\n",
      "0.48786532500115964   17   (10, 10)\n"
     ]
    }
   ],
   "source": [
    "layers=[(15),(15,15),(12,12),(13,13),(14,14),(16,16),(10,10)]\n",
    "nb=[17]\n",
    "\n",
    "for i in layers:\n",
    "    for j in nb:\n",
    "        mlp_regression(X1_val,Y1_val,j,i)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\nscaler_bis = StandardScaler(copy=True,with_mean=True,with_std=True)\\nX1_normalized = scaler_bis.fit_transform(X1_val)\\n\\npca_bis = PCA(n_components=X1.shape[-1])\\n\\ndata_transformed = pca_bis.fit_transform(X1_normalized)\\neig_val = pca_bis.explained_variance_\\neig_vec = pca_bis.components_\\n\\n# Compute an array E, where E(P) indicates the variance captured in the first P component.\\nE = np.array([eig_val[:p+1].sum()/eig_val.sum() for p in range(len(eig_val))])\\ntau = 0.95 # Threshold\\n\\n# Find the minimum P that captures \\tau portion of the variance\\nP = np.where(E>tau)[0][0] +1\\n\\nprint('Minimum number of components that preserve {} of the variance = {} \\n' .format(tau,P))\\nfig = plt.figure(figsize=(20,5))\\nax1 = fig.add_subplot() #(131)\\nax1.plot(np.arange(1,E.shape[0]+1), E, 'o-', markersize=8, color='blue', alpha=0.5)\\nax1.set_xlabel('number of components')\\nax1.set_ylabel('Preserved variance')\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 29
    }
   ],
   "source": [
    "\"\"\"\n",
    "scaler_bis = StandardScaler(copy=True,with_mean=True,with_std=True)\n",
    "X1_normalized = scaler_bis.fit_transform(X1_val)\n",
    "\n",
    "pca_bis = PCA(n_components=X1.shape[-1])\n",
    "\n",
    "data_transformed = pca_bis.fit_transform(X1_normalized)\n",
    "eig_val = pca_bis.explained_variance_\n",
    "eig_vec = pca_bis.components_\n",
    "\n",
    "# Compute an array E, where E(P) indicates the variance captured in the first P component.\n",
    "E = np.array([eig_val[:p+1].sum()/eig_val.sum() for p in range(len(eig_val))])\n",
    "tau = 0.95 # Threshold\n",
    "\n",
    "# Find the minimum P that captures \\tau portion of the variance\n",
    "P = np.where(E>tau)[0][0] +1\n",
    "\n",
    "print('Minimum number of components that preserve {} of the variance = {} \\n' .format(tau,P))\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "ax1 = fig.add_subplot() #(131)\n",
    "ax1.plot(np.arange(1,E.shape[0]+1), E, 'o-', markersize=8, color='blue', alpha=0.5)\n",
    "ax1.set_xlabel('number of components')\n",
    "ax1.set_ylabel('Preserved variance')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nparameters={\\n\\'learning_rate\\': [\"adaptive\"],\\n\\'hidden_layer_sizes\\':[(20),(30),(35),(20,20),(30,30),(35,35)],\\n\\'alpha\\': [0.001,0.0001,0.00001],\\n\\'activation\\': [\"relu\"]\\n}\\n\\nmlp=MLPRegressor()\\n\\nclf = GridSearchCV(mlp, parameters)\\n\\nclf.fit(X1_train_pca, Y1_train)\\n\\nprint(clf.best_params_)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "source": [
    "\"\"\"\n",
    "parameters={\n",
    "'learning_rate': [\"adaptive\"],\n",
    "'hidden_layer_sizes':[(20),(30),(35),(20,20),(30,30),(35,35)],\n",
    "'alpha': [0.001,0.0001,0.00001],\n",
    "'activation': [\"relu\"]\n",
    "}\n",
    "\n",
    "mlp=MLPRegressor()\n",
    "\n",
    "clf = GridSearchCV(mlp, parameters)\n",
    "\n",
    "clf.fit(X1_train_pca, Y1_train)\n",
    "\n",
    "print(clf.best_params_)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}