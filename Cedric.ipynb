{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import sklearn\n",
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "try:\n",
    "    import seaborn as sns\n",
    "    use_seaborn = True\n",
    "    sns.set()\n",
    "except:\n",
    "    use_seaborn = False\n",
    "\n",
    "showfig = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data \n",
    "X1 = pd.read_csv(\"X1.csv\")\n",
    "Y1 = pd.read_csv(\"Y1.csv\",header=None,names =['shares'])\n",
    "\n",
    "if showfig:\n",
    "    fig = plt.figure(figsize=(6.4*2,4.8*6))\n",
    "    gs  = fig.add_gridspec(nrows=12, ncols=5)\n",
    "    for (i,header) in enumerate(X1.columns):\n",
    "        ax = fig.add_subplot(gs[int(i/5),i%5])\n",
    "        ax.scatter(X1[header],Y1.values, s=5)\n",
    "        ax.set_xlabel(header)\n",
    "    fig.tight_layout()\n",
    "\n",
    "X1_val = X1.values\n",
    "Y1_val = Y1.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score computation\n",
    "def scoref1(ytrue, ypred, th):\n",
    "    return sklearn.metrics.f1_score(ytrue>th, ypred>th)\n",
    "\n",
    "def scoreregression(ytrue, ypred):\n",
    "    scores = [\n",
    "        scoref1(ytrue, ypred, th=th) for th in [ 500, 1400, 5000, 10000]\n",
    "    ]\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#feature selections\n",
    "def correlation_selection(X,Y):\n",
    "    data = np.concatenate((np.transpose(X),np.transpose(Y)))\n",
    "    corr = np.corrcoef(data)\n",
    "    idxs = np.argpartition(corr[:][58], -4)[-4:]\n",
    "    X_corr = X[:,idxs[:-1]]\n",
    "\n",
    "    return X_corr\n",
    "\n",
    "def PCA_selection(X,nb_components):\n",
    "    # To do 1: initiate StandardScaler class\n",
    "    scaler = StandardScaler(copy=True,with_mean=True,with_std=True)\n",
    "    # Initialize PCA\n",
    "    pca = PCA(n_components=nb_components)\n",
    "    X_pca = pca.fit_transform(scaler.fit_transform(X))\n",
    "    \n",
    "    return X_pca\n",
    "\n",
    "def mutual_info_selection(X,Y):\n",
    "    mutual_information = mutual_info_regression(X,Y)\n",
    "    idxs = np.argpartition(mutual_information[:][57], -4)[-4:]\n",
    "    X_mutual_info = X[:,idxs[:-1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression(X,Y):\n",
    "    X_corr=correlation_selection(X,Y)\n",
    "\n",
    "    X1_train_corr, X1_test_corr, Y1_train_corr, Y1_test_corr = train_test_split(X_corr,Y1_val,random_state=1,test_size=0.3)\n",
    "\n",
    "    regr = linear_model.LinearRegression()\n",
    "    regr.fit(X1_train_corr, Y1_train_corr)\n",
    "    Y1_pred = regr.predict(X1_test_corr)\n",
    "    \n",
    "    print(scoreregression(Y1_test_corr,Y1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.47776142900688745\n"
     ]
    }
   ],
   "source": [
    "linear_regression(X1_val,Y1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_regression(X):\n",
    "    X_knn=PCA_selection(X1_val,5)\n",
    "\n",
    "    X1_train_pca,X1_test_pca,Y1_train_pca,Y1_test_pca = train_test_split(X_knn,Y1_val,random_state=1,test_size=0.3)\n",
    "\n",
    "    knn = KNeighborsRegressor(20)\n",
    "    #knn.fit(X1_train_pca, Y1_train.ravel())\n",
    "    knn.fit(X1_train_pca, Y1_train_pca)\n",
    "    Y1_pred = knn.predict(X1_test_pca)\n",
    "\n",
    "    print(scoreregression(Y1_test_pca, Y1_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.4954803984890093\n"
     ]
    }
   ],
   "source": [
    "knn_regression(X1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_regression(X):\n",
    "    X_mlp=PCA_selection(X1_val,25)\n",
    "\n",
    "    X1_train_pca,X1_test_pca,Y1_train_pca,Y1_test_pca = train_test_split(X_mlp,Y1_val,random_state=1,test_size=0.3)\n",
    "\n",
    "    reg = MLPRegressor(hidden_layer_sizes=(20,20),activation=\"relu\" ,random_state=1, max_iter=200)\n",
    "    reg.fit(X1_train_pca, Y1_train_pca)\n",
    "    Y1_pred=reg.predict(X1_test_pca)\n",
    "\n",
    "    print(scoreregression(Y1_test_pca, Y1_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.48769223361889025\n"
     ]
    }
   ],
   "source": [
    "mlp_regression(X1_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "\"\\nscaler_bis = StandardScaler(copy=True,with_mean=True,with_std=True)\\nX1_normalized = scaler_bis.fit_transform(X1_val)\\n\\npca_bis = PCA(n_components=X1.shape[-1])\\n\\ndata_transformed = pca_bis.fit_transform(X1_normalized)\\neig_val = pca_bis.explained_variance_\\neig_vec = pca_bis.components_\\n\\n# Compute an array E, where E(P) indicates the variance captured in the first P component.\\nE = np.array([eig_val[:p+1].sum()/eig_val.sum() for p in range(len(eig_val))])\\ntau = 0.95 # Threshold\\n\\n# Find the minimum P that captures \\tau portion of the variance\\nP = np.where(E>tau)[0][0] +1\\n\\nprint('Minimum number of components that preserve {} of the variance = {} \\n' .format(tau,P))\\nfig = plt.figure(figsize=(20,5))\\nax1 = fig.add_subplot() #(131)\\nax1.plot(np.arange(1,E.shape[0]+1), E, 'o-', markersize=8, color='blue', alpha=0.5)\\nax1.set_xlabel('number of components')\\nax1.set_ylabel('Preserved variance')\\n\""
      ]
     },
     "metadata": {},
     "execution_count": 11
    }
   ],
   "source": [
    "\"\"\"\n",
    "scaler_bis = StandardScaler(copy=True,with_mean=True,with_std=True)\n",
    "X1_normalized = scaler_bis.fit_transform(X1_val)\n",
    "\n",
    "pca_bis = PCA(n_components=X1.shape[-1])\n",
    "\n",
    "data_transformed = pca_bis.fit_transform(X1_normalized)\n",
    "eig_val = pca_bis.explained_variance_\n",
    "eig_vec = pca_bis.components_\n",
    "\n",
    "# Compute an array E, where E(P) indicates the variance captured in the first P component.\n",
    "E = np.array([eig_val[:p+1].sum()/eig_val.sum() for p in range(len(eig_val))])\n",
    "tau = 0.95 # Threshold\n",
    "\n",
    "# Find the minimum P that captures \\tau portion of the variance\n",
    "P = np.where(E>tau)[0][0] +1\n",
    "\n",
    "print('Minimum number of components that preserve {} of the variance = {} \\n' .format(tau,P))\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "ax1 = fig.add_subplot() #(131)\n",
    "ax1.plot(np.arange(1,E.shape[0]+1), E, 'o-', markersize=8, color='blue', alpha=0.5)\n",
    "ax1.set_xlabel('number of components')\n",
    "ax1.set_ylabel('Preserved variance')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'\\nparameters={\\n\\'learning_rate\\': [\"adaptive\"],\\n\\'hidden_layer_sizes\\':[(20),(30),(35),(20,20),(30,30),(35,35)],\\n\\'alpha\\': [0.001,0.0001,0.00001],\\n\\'activation\\': [\"relu\"]\\n}\\n\\nmlp=MLPRegressor()\\n\\nclf = GridSearchCV(mlp, parameters)\\n\\nclf.fit(X1_train_pca, Y1_train)\\n\\nprint(clf.best_params_)\\n'"
      ]
     },
     "metadata": {},
     "execution_count": 12
    }
   ],
   "source": [
    "\"\"\"\n",
    "parameters={\n",
    "'learning_rate': [\"adaptive\"],\n",
    "'hidden_layer_sizes':[(20),(30),(35),(20,20),(30,30),(35,35)],\n",
    "'alpha': [0.001,0.0001,0.00001],\n",
    "'activation': [\"relu\"]\n",
    "}\n",
    "\n",
    "mlp=MLPRegressor()\n",
    "\n",
    "clf = GridSearchCV(mlp, parameters)\n",
    "\n",
    "clf.fit(X1_train_pca, Y1_train)\n",
    "\n",
    "print(clf.best_params_)\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}